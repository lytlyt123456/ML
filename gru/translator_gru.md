# （一）RNN------循环神经网络

自然语言处理（NLP）任务通常是让模型理解给定的自然语言文本序列，并根据理解输出新的自然语言文本序列。一段自然语言文本可以被划分成不同的单词，每个单词的含义既取决于其本身，也取决于其所在的上下文。因此，对于给定的文本序列，模型在理解每个单词时，通常要结合单词本身和其所在的上下文来理解。这样的模型称为序列模型。

序列模型的架构通常由一个编码器和一个解码器构成。编码器的输入是自然语言序列，输出是编码得到的一组词嵌入，原始序列中的每个单词对应编码后的一个词嵌入。每个词嵌入即为模型对单词的理解，其不仅包含对应单词本身的信息，还包含单词所在上下文的信息。解码器的输入是编码得到的词嵌入，任务是根据这组词嵌入生成新的自然语言文本序列。对于机器翻译任务而言，生成的新的自然序言序列就是原始序列的译文。

对于一段自然语言文本，人脑的理解过程通常是这样：

从左到右阅读这段自然语言文本，对于读到的每个单词，人脑都会根据当前的单词和之前阅读到的所有内容产生自己对这个单词的理解，直到读完整段文本。

我们可以根据人脑的理解过程，进行如下建模：

**模型输入：**

- 模型对前面阅读过的所有内容的理解 $h_{i - 1}$；
- 与当前读到的单词 $x_{i}$。

**模型输出：**

- 模型根据新输入的单词更新之前的理解，得到新的理解 $h_{i}$。
- 最终，$h_{i}$ 经过线性变换，得到词嵌入 $e_{i}$。

用数学公式表达上述过程，即：

$$
h_{i} = f_{1}(h_{i - 1},x_{i})
$$

$$
e_{i} = f_{2}(h_{i})
$$

我们可以进一步细化这一过程：

$$
h_{i} = \tanh\left( \left( x_{i}U + b_{1} \right) + (h_{i - 1}W + b_{2}) \right)
$$

$$
e_{i} = h_{i}V + b_{3}
$$

其中 $\tanh$ 为激活函数。$h_{i}$ 向量称为隐藏状态。

上述模型即为循环神经网络。

---

# （二）GRU------门控循环单元

RNN 有一个明显的缺陷：对于较长的自然语言文本序列，随着新单词不断输入进模型，序列中较早出现的单词就容易被模型遗忘，模型对序列中靠后的单词的记忆则更加清晰。这会导致模型倾向于忽略前面的单词所传递的信息，而对后面的单词给予更多的关注。

因此，需要对模型进行优化，以缓解上述问题。

在 RNN 中，模型根据新输入的单词 $x_{i}$ 更新其原来的隐藏状态 $h_{i - 1}$，得到新的隐藏状态 $h_{i}$。考虑下面两个问题：

（1）$h_{i - 1}$ 中，是否每个维度的信息都应该被关注？
（2）$h_{i - 1}$ 中，是否每个维度的信息都需要被新输入的单词 $x_{i}$ 来更新？

对于第（1）个问题，我们需要让模型有重点地提取前一个隐藏状态的信息，这样可以让模型对前面隐藏状态中重要的信息给予充分的关注，缓解模型在后期遗忘文本序列中前段内容的问题。

对于第（2）个问题，每个维度的信息都被新输入的单词更新，容易导致前面隐藏状态中的关键信息被后面新的单词所覆盖。我们应让模型识别出 $h_{i - 1}$ 中的哪些维度的信息需要被更新，哪些维度的信息需要保留。

在 GRU 中，提供了两个门------重置门和更新门。

其中，**重置门**用于识别 $h_{i - 1}$ 中哪些维度的信息需要保留，哪些维度的信息可以被遗忘。其实现如下：

$$
r_{i} = \text{sigmoid}\left( \left( x_{i}U_{r} + b_{r}^{(1)} \right) + \left( h_{i - 1}W_{r} + b_{r}^{(2)} \right) \right)
$$

$r_{i}$ 为重置门的输出。值接近 0 的维度，表示该维度的信息更多应被遗忘；值接近 1 的维度，表示该维度的信息更多应被保留。

**更新门**用于识别 $h_{i - 1}$ 中哪些维度的信息需要被更新，哪些维度的信息需要被保持。其实现如下：

$$
z_{i} = \text{sigmoid}\left( \left( x_{i}U_{z} + b_{z}^{(1)} \right) + \left( h_{i - 1}W_{z} + b_{z}^{(2)} \right) \right)
$$

$z_{i}$ 为更新门的输出。值接近 0 的维度，表示该维度的信息更多应被保持；值接近 1 的维度，表示该维度的信息更多应被更新。

首先，根据 $x_{i}$、$h_{i - 1}$ 和重置门的输出 $r_{i}$ 得到候选隐藏状态 $h_{i}'$：

$$
h_{i}' = \tanh\left( \left( x_{i}U + b_{1} \right) + ((h_{i - 1}\bigodot r_{i})W + b_{2}) \right)
$$

然后根据候选隐藏状态 $h_{i}'$、前一个隐藏状态 $h_{i - 1}$ 和更新门的输出 $z_{i}$ 得到新的隐藏状态 $h_{i}$：

$$
h_{i} = \left( 1 - z_{i} \right)\bigodot h_{i - 1} + z_{i}\bigodot h_{i}'
$$

最后，根据新的隐藏状态 $h_{i}$ 得到词嵌入 $e_{i}$：

$$
e_{i} = h_{i}V + b_{3}
$$

---

# （三）GRU 实现机器翻译任务

机器翻译模型分为编码器和解码器。

## 1、编码器

编码器根据输入的自然语言序列生成一组词嵌入。上面对 GRU 模型原理的说明，即大致展示了机器翻译模型编码器的结构。每次接收新输入的单词，根据新输入的单词和前一个隐藏状态得到新的隐藏状态，再根据新的隐藏状态得到词嵌入。

此处需要解释的问题是如何将自然语言序列中的每个单词转化为一个数值向量。本实验中采用如下方式实现：

首先对数据集进行分词（本实验对英文按照空格进行分词，对中文则简化处理，每个字作为一个单词），得到词表。词表中每个单词对应一个编号。除了这些单词之外，词表中还要加入 \<SOS\> token 和 \<EOS\> token。\<SOS\> 是序列开始的标志，\<EOS\> 是序列结束的标志。

设词表大小为 $N$，则设置 $N$ 个可学习的向量，每个向量对应一个单词，作为这个单词的数值向量表示。向量的值通过学习得到。依次将每个单词对应的可学习向量输入进 GRU 编码器，将单词编码为词嵌入。最后输入进编码器的单词为 \<EOS\>，表示序列结束。（注意编码器起始输入的单词为自然语言序列中的第一个单词，不是 \<SOS\>，\<SOS\> 用于解码器）

## 2、解码器

解码器根据编码器得到的词嵌入生成原始文本的译文。

首先，\<SOS\> 和编码器最后一层的隐藏状态输入进 GRU，得到的输出经过线性层分类头得到一个 $M$ 维向量，其中 $M$ 为目标语言的词表大小。该向量经过 softmax 函数得到一个概率分布，概率值最大的维度对应的单词即为模型生成译文的第一个单词。接着，模型生成的第一个单词对应的可学习数值向量和解码器第一层的隐藏状态输入进 GRU，得到的输出为模型生成译文的第二个单词。以此类推，直到模型输出 \<EOS\>，解码过程结束。整个解码过程中生成的文本序列即为原始文本的译文。

在阅读完原始文本后，在解码阶段，模型难免对原始文本中的内容有所遗忘。因此相比于编码器，解码器部分引入注意力机制。模型需要根据当前层输入的单词（即前一层输出的单词）$x_{i}$ 和前一层的隐藏状态 $h_{i - 1}$ 来得到当前层对原始文本中每个单词的注意力分数。具体实现如下：

首先根据 $x_{i}$ 和 $h_{i - 1}$ 得到当前层的查询 $q_{i}$：

$$
q_{i} = \text{CONCAT}\left( x_{i},h_{i - 1} \right)W_{q} + b_{q}
$$

然后求查询 $q_{i}$ 和编码得到的每个单词的词嵌入的余弦相似度，再经过 softmax 函数，得到注意力分数：

$$
\text{attn} = \text{softmax}\left( q_{i}e^{T} \right)
$$

再将 $\text{attn}$ 与词嵌入矩阵 $e$ 相乘，得到本层需要关注的原始文本中的信息（相当于对每个词嵌入按照注意力分数求加权平均）：

$$
\text{info} = \text{attn} \bullet e
$$

$x_{i}$ 和 $\text{info}$ 拼接后的向量经过线性层和激活函数，作为 GRU 的输入。

## 3、训练方法

解码器每一层的输出经过线性分类头和 softmax 函数得到单词预测的概率分布，通过交叉熵损失进行训练即可。

训练需要注意的问题是：在训练前期，解码器中将前一层的输出单词作为下一层的输入单词是不妥的，因为前一层的输出单词可能与标准译文相差甚远，这会导致模型根据一个完全错误的单词来预测下一个单词，从而学习到错误的预测逻辑。因此在训练前期，应采用教师驱动的方式，即将标准译文中的单词作为解码器的输入，而不是上一层的输出。在模型训练后期，可以撤销教师驱动，让模型学会根据前一层的输出单词来预测下一个单词。